{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: -c: line 0: syntax error near unexpected token `('\n",
      "/bin/sh: -c: line 0: `caffeinate -t (3600 * 37)'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1128 20:22:11.132919 4509007296 deprecation.py:323] From /Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1128 20:22:11.411345 4509007296 deprecation_wrapper.py:119] From /Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1, score: 7.0\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m_ctypes/callbacks.c\u001b[0m in \u001b[0;36m'calling callback function'\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36mobjc_method\u001b[0;34m(objc_self, objc_cmd, *args)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0mpy_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjc_cmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_method_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObjCClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/pyglet/window/cocoa/pyglet_window.py\u001b[0m in \u001b[0;36mnextEventMatchingMask_untilDate_inMode_dequeue_\u001b[0;34m(self, mask, date, mode, dequeue)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mPygletWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'@'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mNSUIntegerEncoding\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mb'@@B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnextEventMatchingMask_untilDate_inMode_dequeue_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdequeue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minLiveResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Call the idle() method while we're stuck in a live resize event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;34m\"\"\"Call the method with the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "!caffeinate -t (3600 * 37)\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "from collections import deque\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "f_train_dir = 'tf_train_breakout'\n",
    "f_restore_file_path = '/Users/catherinejohnson/projects/CSCI_3202/deepQNetwork/tf_train_breakout/breakout_model_20201128034415.h5'\n",
    "f_num_episode = 100000\n",
    "f_observe_step_num = 50000\n",
    "f_epsilon_step_num = 1000000\n",
    "f_refresh_target_model_num = 10000\n",
    "f_replay_memory = 400000\n",
    "f_no_op_steps = 30\n",
    "f_regularizer_scale = 0.01\n",
    "f_batch_size = 32\n",
    "f_learning_rate = 0.00025\n",
    "f_init_epsilon = 1.0\n",
    "f_final_epsilon = 0.1\n",
    "f_gamma = 0.99\n",
    "f_resume = False\n",
    "f_render = False\n",
    "\n",
    "ATARI_SHAPE = (84, 84, 4)  # input image size to model\n",
    "ACTION_SIZE = 3\n",
    "\n",
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "\n",
    "# 210*160*3(color) --> 84*84(mono)\n",
    "# float --> integer (to reduce the size of replay memory)\n",
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(downsample(img))\n",
    "\n",
    "def huber_loss(y, q_value):\n",
    "    error = K.abs(y - q_value)\n",
    "    quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "    linear_part = error - quadratic_part\n",
    "    loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def atari_model():\n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = layers.Input(ATARI_SHAPE, name='frames')\n",
    "    actions_input = layers.Input((ACTION_SIZE,), name='action_mask')\n",
    "\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    normalized = layers.Lambda(lambda x: x / 255.0, name='normalization')(frames_input)\n",
    "\n",
    "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = layers.convolutional.Conv2D(\n",
    "        16, (8, 8), strides=(4, 4), activation='relu'\n",
    "    )(normalized)\n",
    "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = layers.convolutional.Conv2D(\n",
    "        32, (4, 4), strides=(2, 2), activation='relu'\n",
    "    )(conv_1)\n",
    "    # Flattening the second convolutional layer.\n",
    "    conv_flattened = layers.core.Flatten()(conv_2)\n",
    "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "    hidden = layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = layers.Dense(ACTION_SIZE)(hidden)\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    filtered_output = layers.Multiply(name='QValue')([output, actions_input])\n",
    "\n",
    "    model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "    model.summary()\n",
    "    optimizer = RMSprop(lr=f_learning_rate, rho=0.95, epsilon=0.01)\n",
    "    # model.compile(optimizer, loss='mse')\n",
    "    # to changed model weights more slowly, uses MSE for low values and MAE(Mean Absolute Error) for large values\n",
    "    model.compile(optimizer, loss=huber_loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "# get action from model using epsilon-greedy policy\n",
    "# cj: annealing\n",
    "def get_action(history, epsilon, step, model):\n",
    "    if np.random.rand() <= epsilon or step <= f_observe_step_num:\n",
    "        return random.randrange(ACTION_SIZE)\n",
    "    else:\n",
    "        q_value = model.predict([history, np.ones(ACTION_SIZE).reshape(1, ACTION_SIZE)])\n",
    "    return np.argmax(q_value[0]) + 1\n",
    "\n",
    "\n",
    "# save sample <s,a,r,s'> to the replay memory\n",
    "def store_memory(memory, history, action, reward, next_history, dead):\n",
    "    memory.append((history, action, reward, next_history, dead))\n",
    "\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    # array for hot mapping each action\n",
    "    # catj: todo: look at targets, classes, and shape (I think three classes)\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "\n",
    "\n",
    "# train model by radom batch\n",
    "def train_memory_batch(memory, model, log_dir):\n",
    "    mini_batch = random.sample(memory, f_batch_size)\n",
    "    history = np.zeros((f_batch_size, ATARI_SHAPE[0],\n",
    "                        ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    next_history = np.zeros((f_batch_size, ATARI_SHAPE[0],\n",
    "                             ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    target = np.zeros((f_batch_size,))\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    # catj: memory is stored in:\n",
    "        # catj: for indices 0 = history, 1 = action, 2 = reward, 3 = next_history, 4 = dead\n",
    "    for idx, val in enumerate(mini_batch):\n",
    "        history[idx] = val[0]\n",
    "        next_history[idx] = val[3]\n",
    "        action.append(val[1])\n",
    "        reward.append(val[2])\n",
    "        dead.append(val[4])\n",
    "\n",
    "    actions_mask = np.ones((f_batch_size, ACTION_SIZE))\n",
    "    # catj: predict for each action since mask is all 1s.\n",
    "    next_Q_values = model.predict([next_history, actions_mask])\n",
    "\n",
    "    # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    for i in range(f_batch_size):\n",
    "        if dead[i]:\n",
    "            target[i] = -1\n",
    "            # target[i] = reward[i]\n",
    "        else:\n",
    "            # catj: Q(s, a) = r + gamma * max(Q(s', a'))\n",
    "            target[i] = reward[i] + f_gamma * np.amax(next_Q_values[i])\n",
    "\n",
    "    # catj get an action for each possible reward.\n",
    "    action_one_hot = get_one_hot(action, ACTION_SIZE)\n",
    "    # catj map each action to reward\n",
    "    target_one_hot = action_one_hot * target[:, None]\n",
    "\n",
    "    #tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "    #                          write_graph=True, write_images=False)\n",
    "\n",
    "    ''''''\n",
    "    h = model.fit(\n",
    "        [history, action_one_hot], target_one_hot, epochs=1,\n",
    "        batch_size=f_batch_size, verbose=0)\n",
    "        #batch_size=FLAGS.batch_size, verbose=0, callbacks=[tb_callback])\n",
    "\n",
    "    #if h.history['loss'][0] > 10.0:\n",
    "    #    print('too large')\n",
    "\n",
    "    return h.history['loss'][0]\n",
    "\n",
    "def init_history(observe):\n",
    "    # At start of episode, there is no preceding frame\n",
    "    # So just copy initial states to make history\n",
    "    # TODO state = preprocess(observe)\n",
    "    state = pre_processing(observe)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], (1, 84, 84, 4))\n",
    "    return history\n",
    "\n",
    "def find_state_and_history(observe, history):\n",
    "    next_state = pre_processing(observe)\n",
    "    next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "    next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "    return (next_state, next_history)\n",
    "\n",
    "def update_epsilon(global_step, epsilon):\n",
    "    exploration = (global_step > f_observe_step_num)\n",
    "    epsilon_declining = epsilon > f_final\n",
    "    if epsilon_declining and exploration:\n",
    "        epsilon -= epsilon_decay\n",
    "    return epsilon\n",
    "\n",
    "def q_iteration(env, model, state, iteration, memory):\n",
    "    # Choose epsilon based on the iteration\n",
    "    epsilon = update_epsilon(global_step, epsilon)\n",
    "\n",
    "    # Choose the action \n",
    "    action = get_action(history, epsilon, global_step, model_target)\n",
    "\n",
    "    # Play one game iteration (note: according to the next paper, you should actually play 4 times here)\n",
    "    observe, reward, done, info = env.step(real_action)\n",
    "    \n",
    "    # state housekeeping\n",
    "    next_state = pre_processing(observe)\n",
    "    next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "    next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "    if start_life > info['ale.lives']:\n",
    "        dead = True\n",
    "        start_life = info['ale.lives']\n",
    "    \n",
    "    memory.append((history, action, reward, next_history, dead))\n",
    "    \n",
    "    has_reached_training_threshold = (global_step > f_observe_step_num)\n",
    "    refresh_weights = (global_step % f_refresh_target_model_num == 0)\n",
    "    \n",
    "    if has_reached_training_threshold:\n",
    "        model_loss = train_memory_batch(memory, model, log_dir)\n",
    "        loss += model_loss\n",
    "        if refresh_weights:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            \n",
    "    score = score + reward\n",
    "    return score\n",
    "\n",
    "    # Sample and fit\n",
    "#     batch = memory.sample_batch(32)\n",
    "#     fit_batch(model, batch)\n",
    "    \n",
    "def log_stuff(model, global_step, episode_number, score, loss, step, memory, file_writer):\n",
    "    if global_step <= f_observe_step_num:\n",
    "        state = \"observe\"\n",
    "    elif f_observe_step_num < global_step <= f_observe_step_num + f_epsilon_step_num:\n",
    "        state = \"explore\"\n",
    "    else:\n",
    "        state = \"train\"\n",
    "    if episode_number % 100 == 0:\n",
    "        print('state: {}, episode: {}, score: {}, global_step: {}, avg loss: {}, step: {}, memory length: {}'\n",
    "              .format(state, episode_number, score, global_step, loss / float(step), step, len(memory)))\n",
    "\n",
    "    if episode_number % 100 == 0 or (episode_number + 1) == f_num_episode:\n",
    "    #if episode_number % 1 == 0 or (episode_number + 1) == FLAGS.num_episode:  # debug\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        file_name = \"breakout_model_{}.h5\".format(now)\n",
    "        model_path = os.path.join(f_train_dir, file_name)\n",
    "        model.save(model_path)\n",
    "\n",
    "    # Add user custom data to TensorBoard\n",
    "    loss_summary = tf.Summary(\n",
    "        value=[tf.Summary.Value(tag=\"loss\", simple_value=loss / float(step))])\n",
    "    file_writer.add_summary(loss_summary, global_step=episode_number)\n",
    "\n",
    "    score_summary = tf.Summary(\n",
    "        value=[tf.Summary.Value(tag=\"score\", simple_value=score)])\n",
    "    if score >= 5:\n",
    "        print('SCORE: ', score)\n",
    "    file_writer.add_summary(score_summary, global_step=episode_number)\n",
    "\n",
    "\n",
    "def train():\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "    # deque: Once a bounded length deque is full, when new items are added,\n",
    "    # a corresponding number of items are discarded from the opposite end\n",
    "    memory = deque(maxlen=f_replay_memory)\n",
    "    episode_number = 0\n",
    "    epsilon = f_init_epsilon\n",
    "    epsilon_decay = (f_init_epsilon - f_final_epsilon) / f_epsilon_step_num\n",
    "    global_step = 0\n",
    "\n",
    "    if f_resume:\n",
    "        model = load_model(l_restore_file_path)\n",
    "        # Assume when we restore the model, the epsilon has already decreased to the final value\n",
    "        epsilon = l_final_epsilon\n",
    "    else:\n",
    "        model = atari_model()\n",
    "\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    log_dir = \"{}/run-{}-log\".format(f_train_dir, now)\n",
    "    file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "    model_target = clone_model(model)\n",
    "    model_target.set_weights(model.get_weights())\n",
    "\n",
    "    while episode_number < f_num_episode:\n",
    "\n",
    "        done = False\n",
    "        dead = False\n",
    "        # 1 episode = 5 lives\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        loss = 0.0\n",
    "        observe = env.reset()\n",
    "\n",
    "        # this is one of DeepMind's idea.\n",
    "        # just do nothing at the start of episode to avoid sub-optimal\n",
    "        for _ in range(random.randint(1, f_no_op_steps)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "        history = init_history(observe)\n",
    "\n",
    "        while not done:\n",
    "            if f_render:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "            #TODO q_iteration(env, model, state, iteration, memory)  START\n",
    "            \n",
    "            score = q_iteration(env, model, state, iteration, memory)\n",
    "\n",
    "#             # get action for the current history and go one step in environment\n",
    "#             # catj: history builds with each iteration based on next step which gets next\n",
    "#             # state based on action.\n",
    "#             action = get_action(history, epsilon, global_step, model_target)\n",
    "#             # print('action: ', action)\n",
    "#             # change action to real_action\n",
    "#             # catj: TODO: move this to get action to offset action from set.\n",
    "#             real_action = action + 1\n",
    "\n",
    "#             # scale down epsilon, the epsilon only begin to decrease after observe steps\n",
    "#             # catj: anneal epsilon to reduce the amount of exploration.\n",
    "#             exploration = (global_step > f_observe_step_num)\n",
    "#             epsilon_declining = epsilon > f_final\n",
    "#             if epsilon > f_final_epsilon and exploration:\n",
    "#                 epsilon -= epsilon_decay\n",
    "\n",
    "#             observe, reward, done, info = env.step(real_action)\n",
    "#             # pre-process the observation --> history\n",
    "#             # TODO next_state = preprocess(observe)\n",
    "#             next_state = pre_processing(observe)\n",
    "#             next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "#             next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "#             # if the agent missed ball, agent is dead --> episode is not over\n",
    "#             if start_life > info['ale.lives']:\n",
    "#                 dead = True\n",
    "#                 start_life = info['ale.lives']\n",
    "\n",
    "#             # TODO: may be we should give negative reward if miss ball (dead)\n",
    "#             # reward = np.clip(reward, -1., 1.)  # clip here is not correct\n",
    "\n",
    "#             # save the statue to memory, each replay takes 2 * (84*84*4) bytes = 56448 B = 55.125 KB\n",
    "#             store_memory(memory, history, action, reward, next_history, dead)  #\n",
    "\n",
    "#             # check if the memory is ready for training\n",
    "#             if global_step > f_observe_step_num:\n",
    "#                 loss = loss + train_memory_batch(memory, model, log_dir)\n",
    "#                 # if loss > 100.0:\n",
    "#                 #    print(loss)\n",
    "#                 if global_step % f_refresh_target_model_num == 0:  # update the target model\n",
    "#                     model_target.set_weights(model.get_weights())\n",
    "\n",
    "#             # TODO score = q_iteration(env, model, state, iteration, memory) END\n",
    "#             score += reward\n",
    "            \n",
    "            if not dead:\n",
    "                history = next_history\n",
    "\n",
    "            # iterative house keeping resets for next iteration\n",
    "            dead = False\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                log_stuff(model, global_step, episode_number, score, loss, step, memory, file_writer)\n",
    "                episode_number += 1\n",
    "\n",
    "    file_writer.close()\n",
    "\n",
    "\n",
    "def test():\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "    episode_number = 0\n",
    "    epsilon = 0.001\n",
    "    global_step = f_observe_step_num+1\n",
    "    #model = load_model(f_restore_file_path)\n",
    "    model = load_model(f_restore_file_path, custom_objects={'huber_loss': huber_loss})  # load model with customized loss func\n",
    "\n",
    "    # test how to deep copy a model\n",
    "    '''\n",
    "    model_copy = clone_model(model)    # only copy the structure, not the value of the weights\n",
    "    model_copy.set_weights(model.get_weights())\n",
    "    '''\n",
    "\n",
    "    while episode_number < f_num_episode:\n",
    "\n",
    "        # init variables\n",
    "        done = False\n",
    "        dead = False\n",
    "        # 1 episode = 5 lives\n",
    "        score, start_life = 0, 5\n",
    "        observe = env.reset()\n",
    "\n",
    "        observe, _, _, _ = env.step(1)\n",
    "        # At start of episode, there is no preceding frame\n",
    "        # So just copy initial states to make history\n",
    "        # TODO state = preprocess(observe)\n",
    "        history = init_history(observe)\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            time.sleep(0.01)\n",
    "\n",
    "            # get action for the current history and go one step in environment\n",
    "            action = get_action(history, epsilon, global_step, model)\n",
    "\n",
    "            observe, reward, done, info = env.step(action)\n",
    "            # pre-process the observation --> history\n",
    "            next_state, next_history = find_state_and_history(observe, history)\n",
    "\n",
    "            # if the agent has died it should be punished and the next life recorded.\n",
    "            curr_life = info['ale.lives']\n",
    "            new_life = start_life > curr_life\n",
    "            if new_life:\n",
    "                dead = True\n",
    "                start_life = curr_life\n",
    "                # added\n",
    "                reward = -1\n",
    "\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            # resets\n",
    "            if not dead:\n",
    "                history = next_history\n",
    "            dead = False\n",
    "            global_step += 1\n",
    "            if done:\n",
    "                episode_number += 1\n",
    "                print('episode: {}, score: {}'.format(episode_number, score))\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    #train()\n",
    "    test()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/catherinejohnson/Downloads/Project Announcement-20201115\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
