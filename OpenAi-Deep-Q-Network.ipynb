{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This architectural CNN is from the mind of Deep Mind.\n",
    "def cnn_model():\n",
    "    # This model building approach uses the Keras Functional API.\n",
    "    # The Keras Functional API can be found at: https://keras.io/guides/functional_api/\n",
    "    image_framework = layers.Input(ATARI_IMAGE_SHAPE, name='image-shape-framework')\n",
    "    # Normalize 0-255 image scale to within the constraints of 0 and 1.\n",
    "    # This is done last minute because it is a heavy load to store normalized images, but they\n",
    "    # can more easily be transformed into processing by the model.\n",
    "    normalized_images = layers.Lambda(lambda x: x / 255.0, name='normalize-images')(image_framework)\n",
    "\n",
    "    layer_one_hidden = layers.convolutional.Conv2D(\n",
    "        LAYER_1_SIZE, LAYER_1_FILTER, strides=LAYER_1_STRIDES, activation=ACTIVATION_FUNCTION\n",
    "    )(normalized_images)\n",
    "    layer_two_hidden = layers.convolutional.Conv2D(\n",
    "        LAYER_2_SIZE, LAYER_2_FILTER, strides=LAYER_2_STRIDES, activation=ACTIVATION_FUNCTION\n",
    "    )(layer_one_hidden)\n",
    "    # Flatten before connecting to move to 1D structure.\n",
    "    flat_layer = layers.core.Flatten()(layer_two_hidden)\n",
    "    # Dense layer, fully connected.\n",
    "    fully_connected_layer = layers.Dense(256, activation=ACTIVATION_FUNCTION)(flat_layer)\n",
    "    # Dense layer is fully connected.\n",
    "    action_layers = layers.Dense(ACTION_OPTION_COUNT)(fully_connected_layer)\n",
    "    # Action mask encodes.\n",
    "    action_input = layers.Input((ACTION_OPTION_COUNT,), name='action-mask')\n",
    "    # Multiply layer for each action using the encoded action mask.\n",
    "    mult_res_layer = layers.Multiply(name='deep-q-cnn')([action_layers, action_input])\n",
    "\n",
    "    model = Model(inputs=[image_framework, action_input], outputs=mult_res_layer)\n",
    "    model.summary()\n",
    "    # RMSProp is commonly used for minibatch learning.\n",
    "    optimizer = RMSprop(lr=LEARNING_RATE, rho=0.95, epsilon=0.01)\n",
    "    model.compile(optimizer, loss=huber_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir():\n",
    "    curr_time = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    record_dir = \"{}/openai-time-{}-logger\".format(TRAIN_DIR, curr_time)\n",
    "\n",
    "def init_file_writer_to_local_dir():\n",
    "    return tf.summary.FileWriter(get_log_dir(), tf.get_default_graph())\n",
    "\n",
    "def init_batch_matrix():\n",
    "    return np.zeros((BATCH_SIZE, ATARI_IMAGE_SHAPE[0], ATARI_IMAGE_SHAPE[1], ATARI_IMAGE_SHAPE[2]))\n",
    "\n",
    "def preprocess(observe):\n",
    "    grayscale_imgs = rgb2gray(observe)\n",
    "    shrink_imgs = resize(grayscale_imgs, (84, 84), mode='constant')\n",
    "    processed_observe = np.uint8(shrink_imgs * 255)\n",
    "    return processed_observe\n",
    "\n",
    "def init_history(observe):\n",
    "    # At start of game, there is no preceding frame.\n",
    "    # So just copy initial states to make history.\n",
    "    state = preprocess(observe)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], (1, 84, 84, 4))\n",
    "    return history\n",
    "\n",
    "def init_test_config(selected_env):\n",
    "    env = gym.make(selected_env)\n",
    "    player_games = 0\n",
    "    episode_number = 0\n",
    "    epsilon = 0.001\n",
    "    global_step = NUM_OBSERVABLE_STEPS + 1\n",
    "    model = load_model(LOGS_FILE_PATH, custom_objects={'huber_loss': huber_loss})\n",
    "    return (env, player_games, episode_number, epsilon, global_step, model)\n",
    "\n",
    "def init_model_clone(model):\n",
    "    # Copy model since actual model weights will get updated later TODO.  when?\n",
    "    # Clone model using keras api function.\n",
    "    model_clone = clone_model(model)\n",
    "    # Clone model weights to new model separately\n",
    "    model_clone.set_weights(model.get_weights())\n",
    "    return model_clone\n",
    "\n",
    "def init_config(selected_env):\n",
    "    env = gym.make(selected_env)\n",
    "    # Deque is imported from collections.  Set to a finite size.  New memory will overwrite old.\n",
    "    memory = deque(maxlen=400000)\n",
    "    # init\n",
    "    epsilon = 1.0\n",
    "    total_steps = 0\n",
    "    # Init at 0.\n",
    "    player_game = 0\n",
    "    return (env, memory, epsilon, total_steps, player_game)\n",
    "\n",
    "def init_game_config():\n",
    "    done = False\n",
    "    dead = False\n",
    "    game_step = 0\n",
    "    game_score = 0\n",
    "    game_lives = 5\n",
    "    game_loss = 0.0\n",
    "    return (done, dead, game_step, game_score, game_lives, game_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action is random if it is an observed state or if by chance based on the epsilon threshold, it is.\n",
    "# If action is not random it gets generated from the current model based on history data to this point.\n",
    "# I select the best action from this result.\n",
    "def get_action(history, epsilon, model, is_in_observed_state):\n",
    "    # If is_in_observed_state go with random action, otherwise predict it from the model.\n",
    "    is_below_epsilon_threshole = np.random.rand() <= epsilon\n",
    "    if is_below_epsilon_threshole or is_in_observed_state:\n",
    "        return random.randrange(ACTION_OPTION_COUNT)\n",
    "    else:\n",
    "        q_value = model.predict([history, np.ones(ACTION_OPTION_COUNT).reshape(1, ACTION_OPTION_COUNT)])\n",
    "    # Offset for 0 indexing of one-hot encoding array location of value\n",
    "    return np.argmax(q_value[0]) + 1\n",
    "\n",
    "def update_epsilon(total_steps, epsilon):\n",
    "    training = (total_steps > NUM_OBSERVABLE_STEPS)\n",
    "    epsilon_declining = epsilon > 0.1\n",
    "    if epsilon_declining and training:\n",
    "        epsilon -= EPSILON_DECAY\n",
    "    return epsilon\n",
    "\n",
    "def find_state_and_history(observed_state, history):\n",
    "    next_state = preprocess(observed_state)\n",
    "    # next_state = preprocess(observed_state)\n",
    "    next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "    next_history = np.append(next_state, history)\n",
    "    return (next_state, next_history)\n",
    "\n",
    "def find_state_and_history_two(observe, history):\n",
    "    next_state = preprocess(observe)\n",
    "    next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "    next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "    return (next_state, next_history)\n",
    "\n",
    "def update_game_lifecycle(game_lives, info):\n",
    "    game_dead = game_lives > info['ale.lives']\n",
    "    game_lives = info['ale.lives']\n",
    "    return (game_dead, game_lives)\n",
    "\n",
    "def breakout_from_memory(memory):\n",
    "    training_batch = random.sample(memory, BATCH_SIZE)\n",
    "    history = init_batch_matrix()\n",
    "    next_history = init_batch_matrix()\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    # Memory is stored in: indices 0 = history, 1 = action, 2 = reward, 3 = next_history, 4 = dead\n",
    "    for index, val in enumerate(training_batch):\n",
    "        history[index] = val[0]\n",
    "        next_history[index] = val[3]\n",
    "        action.append(val[1])\n",
    "        reward.append(val[2])\n",
    "        dead.append(val[4])\n",
    "        \n",
    "    return (history, next_history, action, reward, dead)\n",
    "\n",
    "def get_one_hot_encoding(targets, nb_classes):\n",
    "    # clip targets to range within 0 and 2 to make sure they are within possibilities.\n",
    "    targets = np.clip(targets, 0, 2)\n",
    "    # array for hot mapping each action\n",
    "    # classes: 3, targets range from 0 to 2.  \n",
    "    # The reshape -1 signifies that data is massaged into a dimension that is unknown.\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "\n",
    "def huber_loss(a, b):\n",
    "    error = K.abs(a - b)\n",
    "    quadratic_term = K.clip(error, 0.0, 1.0)\n",
    "    linear_term = error - quadratic_term\n",
    "    loss = K.mean(0.5 * K.square(quadratic_term) + linear_term)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Method\n",
    "It is not necessary to run this but the training is so long it is useful to check in on how it is performing.\n",
    "Some iterative logging function should be run in case the model quits while the programmer is sleeping or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_log_stuff(model, total_steps, player_game, score, loss, step, memory, file_writer):\n",
    "    # debug.\n",
    "    if player_game % 10 == 0:\n",
    "        print(\"game: {}, total steps: {}, score: {}\".format(player_game, total_steps, score))\n",
    "        \n",
    "    # Record model state iteratively since run time is long.    \n",
    "    if player_game % 1000 == 0 or (player_game + 1) == NUM_TURNS:\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        file_name = \"training_{}.h5\".format(now)\n",
    "        model_path = os.path.join(TRAIN_DIR, file_name)\n",
    "        model.save(model_path)\n",
    "\n",
    "    # Update File for storing model.\n",
    "    loss_summary = tf.Summary(\n",
    "        value=[tf.Summary.Value(tag=\"loss\", simple_value=loss / float(step))])\n",
    "    file_writer.add_summary(loss_summary, global_step=player_game)\n",
    "\n",
    "    score_summary = tf.Summary(\n",
    "        value=[tf.Summary.Value(tag=\"score\", simple_value=score)])\n",
    "    file_writer.add_summary(score_summary, global_step=player_game)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_memory_batch(memory, model, log_dir):\n",
    "    q_s_a = np.zeros((BATCH_SIZE,))\n",
    "    history, next_history, action, reward, dead = breakout_from_memory(memory)\n",
    "\n",
    "    actions_mask = np.ones((BATCH_SIZE, ACTION_OPTION_COUNT))\n",
    "    # catj: predict for each action since mask is all 1s.\n",
    "    next_Q_values = model.predict([next_history, actions_mask])\n",
    "\n",
    "    # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    for i in range(BATCH_SIZE):\n",
    "        if dead[i]:\n",
    "            q_s_a[i] = reward[i]\n",
    "        else:\n",
    "            # Q(s, a) = r + gamma * max(Q(s', a'))\n",
    "            # train model for future reward.\n",
    "            q_s_a[i] = reward[i] + GAMMA * np.amax(next_Q_values[i])\n",
    "\n",
    "    # Get an action for each possible reward.\n",
    "    action_one_hot = get_one_hot_encoding(action, ACTION_OPTION_COUNT)\n",
    "    # action mask on reward array.  mult each action by reward.\n",
    "    # this will be the encoding of each action with an updated Q(s, a)\n",
    "    rewards_one_hot = action_one_hot * q_s_a[:, None]\n",
    "    \n",
    "    # training data is on [history, action_one_hot], classifier is rewards\n",
    "    # in predictions we will take highest reward and map to action using one hot technique.\n",
    "    h = model.fit(\n",
    "        [history, action_one_hot], rewards_one_hot, epochs=1,\n",
    "        batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "    return h.history['loss'][0]\n",
    "\n",
    "# This function parents the deep Q Network if the model has enough memory for batch training.\n",
    "def deep_q_iteration_training(memory, total_steps, model_clone, model):\n",
    "    log_dir = None\n",
    "    loss = train_memory_batch(memory, model, log_dir)\n",
    "    if total_steps % MODEL_WEIGHTS_REFRESH_THRESOLD == 0:\n",
    "        # Weights on the model clone get piped through so they only get updated as often as \n",
    "        # the treshold dictates the cycle update them.\n",
    "        model_clone.set_weights(model.get_weights())\n",
    "    return (model_clone, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mostly this function keeps track of system states, memory, and flags\n",
    "# # It provides the opportunity to create logs for debugging\n",
    "# # Most importantly it takes an action and updates a score.\n",
    "# # It runs training on the model if all observation has been done.  This is Deep Q Learning.\n",
    "def train(selected_env):\n",
    "    # Initialize global states.\n",
    "    env, memory, epsilon, total_steps, player_games = init_config(selected_env)\n",
    "    # Initialize storage for collecting data on model performance.\n",
    "    total_score = 0\n",
    "    avg_game_scores = []\n",
    "    # Get a copy of the cnn model with the architecture defined in a separate function.\n",
    "    model = cnn_model()\n",
    "    # Initialize file writer for logging. \n",
    "    \n",
    "    # Initialize file writer.  \n",
    "    # This is just used for logging and storing the model iteratively to preserve work.\n",
    "    file_writer = init_file_writer_to_local_dir()\n",
    "    \n",
    "    # The main model gets used in the Q learning training, and based on updated weights, \n",
    "    # then also updates the model clone.  \n",
    "    # Targeted Network update.\n",
    "    model_clone = init_model_clone(model)\n",
    "    \n",
    "    # This is just a loop to cover the range of the global number of games played.\n",
    "    # The player games number is kept visible to the program for logging purposes.\n",
    "    while player_games < NUM_TURNS:\n",
    "        # Define global game states.\n",
    "        game_done, player_dead, game_step, game_score, game_lives, game_loss = init_game_config()\n",
    "        # Reset the environment at the beginning of each game.\n",
    "        observe = env.reset()\n",
    "\n",
    "        # Prefill the start state, 4 frames.\n",
    "        for _ in range(random.randint(1, INIT_NO_OP_STEPS)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "        history = init_history(observe)\n",
    "\n",
    "        while not game_done:\n",
    "            is_in_observed_state = (total_steps <= NUM_OBSERVABLE_STEPS)\n",
    "            is_in_training_state = not is_in_observed_state\n",
    "            \n",
    "            # Epsilon has to decay a tiny bit with each iteration in the annealing method.\n",
    "            epsilon = update_epsilon(total_steps, epsilon)\n",
    "\n",
    "            # Get an action\n",
    "            action = get_action(history, epsilon, model_clone, is_in_observed_state)\n",
    "\n",
    "            # Take a step in the game\n",
    "            observed_state, reward, game_done, info = env.step(action)\n",
    "            \n",
    "            # Update score based on agent action.\n",
    "            # Move reward to the poles of 1 or -1 per the deep mind paper's suggestion\n",
    "            game_reward = np.clip(reward, -1., 1.)\n",
    "            game_score += game_reward\n",
    "            \n",
    "            # Preprocess state data and merge it with history.\n",
    "            next_state = preprocess(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            \n",
    "            player_dead, game_lives = update_game_lifecycle(game_lives, info)\n",
    "            \n",
    "            # Update memory.\n",
    "            memory.append((history, action, reward, next_history, player_dead))\n",
    "            \n",
    "            # Deep Q learning begins if the observational state is complete.\n",
    "            # When the model has sufficiently recorded enough memory for training, start batch training.\n",
    "            if is_in_training_state:\n",
    "                model_clone, model_loss = deep_q_iteration_training(memory, total_steps, model_clone, model)\n",
    "                game_loss += model_loss\n",
    "                \n",
    "            if not player_dead:\n",
    "                # Update history to include the state if the agent didn't die.\n",
    "                history = next_history\n",
    "            \n",
    "            # Update counts and state flags.\n",
    "            player_dead = False\n",
    "            # These are used more of less for logging and aren't too important to the system.\n",
    "            total_steps += 1\n",
    "            game_step += 1\n",
    "            \n",
    "            if game_done:\n",
    "                maybe_log_stuff(model, total_steps, player_games, game_score, game_loss, game_step, memory, file_writer)\n",
    "                player_games += 1\n",
    "                # update average game score log\n",
    "                total_score += game_score\n",
    "                # take sample\n",
    "                if player_games % SAMPLE_SCORE_INTERVAL == 0:\n",
    "                    avg_game_score = total_score/SAMPLE_SCORE_INTERVAL\n",
    "                    avg_game_scores.append(avg_game_score)\n",
    "                    total_score = 0\n",
    "    file_writer.close()\n",
    "    return avg_game_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(selected_env):\n",
    "    env = gym.make(selected_env)\n",
    "    player_games = 0\n",
    "    epsilon = 0.001\n",
    "    global_step = NUM_OBSERVABLE_STEPS + 1\n",
    "    model = load_model(LOGS_FILE_PATH, custom_objects={'huber_loss': huber_loss})\n",
    "    total_score = 0\n",
    "    avg_game_scores = []\n",
    "\n",
    "    while player_games < NUM_TURNS:\n",
    "        # init variables\n",
    "        done, player_dead, game_step, game_score, game_lives, game_loss = init_game_config()\n",
    "        \n",
    "        observe = env.reset()\n",
    "\n",
    "        # Copy in initial states to amount to initial four frame history\n",
    "        observe, _, _, _ = env.step(1)\n",
    "        history = init_history(observe)\n",
    "\n",
    "        while not done:\n",
    "            if RENDER is True:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "\n",
    "            is_in_observed_state = (global_step <= NUM_OBSERVABLE_STEPS)\n",
    "            # get action for the current history and go one step in environment\n",
    "            action = get_action(history, epsilon, model, is_in_observed_state)\n",
    "\n",
    "            observe, reward, done, info = env.step(action)\n",
    "            # pre-process the observation --> history\n",
    "            next_state, next_history = find_state_and_history_two(observe, history)\n",
    "\n",
    "            game_dead, game_lives = update_game_lifecycle(game_lives, info)\n",
    "\n",
    "            # move reward to the poles of 1 or -1 per the deep mind paper's suggestion\n",
    "            game_reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            game_score += game_reward\n",
    "\n",
    "            if not game_dead:\n",
    "                # Update history to include the state if the agent didn't die.\n",
    "                history = next_history\n",
    "\n",
    "            # Update counts and state flags.\n",
    "            dead = False\n",
    "            \n",
    "            if done:\n",
    "                player_games += 1\n",
    "                # update average game score log\n",
    "                total_score += game_score\n",
    "                # take sample\n",
    "                if player_games % SAMPLE_SCORE_INTERVAL == 0:\n",
    "                    avg_game_score = total_score/SAMPLE_SCORE_INTERVAL\n",
    "                    avg_game_scores.append(avg_game_score)\n",
    "                    print('avg: ', avg_game_scores)\n",
    "                    total_score = 0\n",
    "    return avg_game_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "from collections import deque\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'openai_breakout_training_storage'\n",
    "LOGS_FILE_PATH = '/Users/catherinejohnson/projects/CSCI_3202/deepQNetwork/openai_breakout_training_storage/breakout_model_20201128034415.h5'\n",
    "# suggested by Deep Mind Paper\n",
    "NUM_TURNS = 100\n",
    "#NUM_TURNS = 100000\n",
    "# suggested by Deep Mind Paper\n",
    "NUM_OBSERVABLE_STEPS = 50\n",
    "# NUM_OBSERVABLE_STEPS = 50000\n",
    "MODEL_WEIGHTS_REFRESH_THRESOLD = 10\n",
    "# MODEL_WEIGHTS_REFRESH_THRESOLD = 10000\n",
    "# suggested by Deep Mind Paper\n",
    "INIT_NO_OP_STEPS = 10\n",
    "# INIT_NO_OP_STEPS = 30\n",
    "REGULATION_SCALE = 0.01\n",
    "# suggested by Deep Mind Paper\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.00025\n",
    "GAMMA = 0.99\n",
    "# suggested by Deep Mind Paper\n",
    "ATARI_IMAGE_SHAPE = (84, 84, 4)\n",
    "EPSILON_DECAY = ((1.0 - 0.1) / 1000000)\n",
    "# suggested by Deep Mind Paper\n",
    "ATARI_IMAGE_SHAPE = (84, 84, 4)\n",
    "ACTION_OPTION_COUNT = 3\n",
    "SAMPLE_SCORE_INTERVAL = 10\n",
    "RENDER = False\n",
    "\n",
    "# suggested by Deep Mind Paper\n",
    "LAYER_1_SIZE = 16\n",
    "LAYER_1_FILTER = (8, 8)\n",
    "LAYER_1_STRIDES = (4, 4)\n",
    "LAYER_2_SIZE = 32\n",
    "LAYER_2_FILTER = (4, 4)\n",
    "LAYER_2_STRIDES = (2, 2)\n",
    "ACTIVATION_FUNCTION = 'relu'\n",
    "\n",
    "ATARI_BREAKOUT_ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "ATARI_SPACE_ENVADERS_ENV_NAME = 'SpaceInvadersDeterministic-v4'\n",
    "ATARI_SKIING_ENV_NAME = 'SkiingDeterministic-v4'\n",
    "ATARI_TENNIS_ENV_NAME = 'TennisDeterministic-v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image-shape-framework (InputLay (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normalize-images (Lambda)       (None, 84, 84, 4)    0           image-shape-framework[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 20, 20, 16)   4112        normalize-images[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 9, 9, 32)     8224        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2592)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          663808      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            771         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action-mask (InputLayer)        (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deep-q-cnn (Multiply)           (None, 3)            0           dense_2[0][0]                    \n",
      "                                                                 action-mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 676,915\n",
      "Trainable params: 676,915\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "game: 0, total steps: 193, score: 2.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-15a58b7b1058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavg_game_scores_breakout_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATARI_BREAKOUT_ENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-5832913ad58a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(selected_env)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# When the model has sufficiently recorded enough memory for training, start batch training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_in_training_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mmodel_clone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_q_iteration_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_clone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mgame_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-03bda6321843>\u001b[0m in \u001b[0;36mdeep_q_iteration_training\u001b[0;34m(memory, total_steps, model_clone, model)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeep_q_iteration_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_clone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mMODEL_WEIGHTS_REFRESH_THRESOLD\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Weights on the model clone get piped through so they only get updated as often as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-03bda6321843>\u001b[0m in \u001b[0;36mtrain_memory_batch\u001b[0;34m(memory, model, log_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mactions_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION_OPTION_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# catj: predict for each action since mask is all 1s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnext_Q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# like Q Learning, get maximum Q value at s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_game_scores_breakout_training = train(ATARI_BREAKOUT_ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg:  [12.5]\n",
      "avg:  [12.5, 12.0]\n"
     ]
    }
   ],
   "source": [
    "# manually update file in RESTORE_FILE_PATH for testing.\n",
    "avg_game_scores_breakout_testing = test(ATARI_BREAKOUT_ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = [avg_game_scores_breakout_training.index(i) for i in avg_game_scores_breakout_training]\n",
    "plt.plot(avg_game_scores_breakout_training)\n",
    "# plt.plot(avg_game_scores_spaci_training)\n",
    "print(avg_game_scores_breakout_training)\n",
    "#plt.plot(time_series)\n",
    "plt.title('Training')\n",
    "plt.ylabel('avg scores')\n",
    "plt.xlabel('time')\n",
    "plt.legend(['breakout', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(avg_game_scores_breakout_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x, y, and format string must not be None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9f55d1505cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_game_scores_breakout_testing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# plt.plot(avg_game_scores_sesh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_game_scores_spaci_testing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mavg_game_scores_sesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavg_game_scores_sesh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2840\u001b[0m     return gca().plot(\n\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/catherinejohnson/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# element array of None which causes problems downstream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x, y, and format string must not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x, y, and format string must not be None"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAANQklEQVR4nO3cX4il9X3H8fenuxEak0aJk5DurmRb1pi90KITI6VpTUObXXuxBLxQQ6QSWKQx5FIpNLnwprkohKBmWWSR3GQvGkk2ZRMplMSCNd1Z8N8qynSlOl3BNYYUDFRWv704p51hnHWenXNmZp3v+wUD85znNzPf+TH73mfPznlSVUiStr7f2ewBJEkbw+BLUhMGX5KaMPiS1ITBl6QmDL4kNbFq8JMcSfJakmfPcz5JvptkPsnTSa6b/piSpEkNucJ/GNj3Huf3A3vGbweB700+liRp2lYNflU9BrzxHksOAN+vkSeAy5J8YloDSpKmY/sUPscO4JUlxwvjx15dvjDJQUb/CuDSSy+9/uqrr57Cl5ekPk6ePPl6Vc2s5WOnEfys8NiK92uoqsPAYYDZ2dmam5ubwpeXpD6S/OdaP3Yav6WzAOxacrwTODOFzytJmqJpBP8YcMf4t3VuBH5TVe96OkeStLlWfUonyQ+Am4ArkiwA3wI+AFBVh4DjwM3APPBb4M71GlaStHarBr+qblvlfAFfm9pEkqR14SttJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJamJQ8JPsS/JCkvkk965w/iNJfpLkqSSnktw5/VElSZNYNfhJtgEPAPuBvcBtSfYuW/Y14Lmquha4CfiHJJdMeVZJ0gSGXOHfAMxX1emqegs4ChxYtqaADycJ8CHgDeDcVCeVJE1kSPB3AK8sOV4YP7bU/cCngTPAM8A3quqd5Z8oycEkc0nmzp49u8aRJUlrMST4WeGxWnb8ReBJ4PeBPwLuT/J77/qgqsNVNVtVszMzMxc4qiRpEkOCvwDsWnK8k9GV/FJ3Ao/UyDzwEnD1dEaUJE3DkOCfAPYk2T3+j9hbgWPL1rwMfAEgyceBTwGnpzmoJGky21dbUFXnktwNPApsA45U1akkd43PHwLuAx5O8gyjp4DuqarX13FuSdIFWjX4AFV1HDi+7LFDS94/A/zldEeTJE2Tr7SVpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDUxKPhJ9iV5Icl8knvPs+amJE8mOZXkF9MdU5I0qe2rLUiyDXgA+AtgATiR5FhVPbdkzWXAg8C+qno5ycfWaV5J0hoNucK/AZivqtNV9RZwFDiwbM3twCNV9TJAVb023TElSZMaEvwdwCtLjhfGjy11FXB5kp8nOZnkjpU+UZKDSeaSzJ09e3ZtE0uS1mRI8LPCY7XseDtwPfBXwBeBv0ty1bs+qOpwVc1W1ezMzMwFDytJWrtVn8NndEW/a8nxTuDMCmter6o3gTeTPAZcC7w4lSklSRMbcoV/AtiTZHeSS4BbgWPL1vwY+FyS7Uk+CHwWeH66o0qSJrHqFX5VnUtyN/AosA04UlWnktw1Pn+oqp5P8jPgaeAd4KGqenY9B5ckXZhULX86fmPMzs7W3NzcpnxtSXq/SnKyqmbX8rG+0laSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmBgU/yb4kLySZT3Lve6z7TJK3k9wyvRElSdOwavCTbAMeAPYDe4Hbkuw9z7pvA49Oe0hJ0uSGXOHfAMxX1emqegs4ChxYYd3XgR8Cr01xPknSlAwJ/g7glSXHC+PH/l+SHcCXgEPv9YmSHEwyl2Tu7NmzFzqrJGkCQ4KfFR6rZcffAe6pqrff6xNV1eGqmq2q2ZmZmYEjSpKmYfuANQvAriXHO4Ezy9bMAkeTAFwB3JzkXFX9aBpDSpImNyT4J4A9SXYD/wXcCty+dEFV7f6/95M8DPyTsZeki8uqwa+qc0nuZvTbN9uAI1V1Ksld4/Pv+by9JOniMOQKn6o6Dhxf9tiKoa+qv558LEnStPlKW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSE4OCn2RfkheSzCe5d4XzX07y9Pjt8STXTn9USdIkVg1+km3AA8B+YC9wW5K9y5a9BPxZVV0D3AccnvagkqTJDLnCvwGYr6rTVfUWcBQ4sHRBVT1eVb8eHz4B7JzumJKkSQ0J/g7glSXHC+PHzuerwE9XOpHkYJK5JHNnz54dPqUkaWJDgp8VHqsVFyafZxT8e1Y6X1WHq2q2qmZnZmaGTylJmtj2AWsWgF1LjncCZ5YvSnIN8BCwv6p+NZ3xJEnTMuQK/wSwJ8nuJJcAtwLHli5IciXwCPCVqnpx+mNKkia16hV+VZ1LcjfwKLANOFJVp5LcNT5/CPgm8FHgwSQA56pqdv3GliRdqFSt+HT8upudna25ublN+dqS9H6V5ORaL6h9pa0kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNDAp+kn1JXkgyn+TeFc4nyXfH559Oct30R5UkTWLV4CfZBjwA7Af2Arcl2bts2X5gz/jtIPC9Kc8pSZrQkCv8G4D5qjpdVW8BR4EDy9YcAL5fI08AlyX5xJRnlSRNYPuANTuAV5YcLwCfHbBmB/Dq0kVJDjL6FwDA/yR59oKm3bquAF7f7CEuEu7FIvdikXux6FNr/cAhwc8Kj9Ua1lBVh4HDAEnmqmp2wNff8tyLRe7FIvdikXuxKMncWj92yFM6C8CuJcc7gTNrWCNJ2kRDgn8C2JNkd5JLgFuBY8vWHAPuGP+2zo3Ab6rq1eWfSJK0eVZ9SqeqziW5G3gU2AYcqapTSe4anz8EHAduBuaB3wJ3Dvjah9c89dbjXixyLxa5F4vci0Vr3otUveupdknSFuQrbSWpCYMvSU2se/C9LcOiAXvx5fEePJ3k8STXbsacG2G1vViy7jNJ3k5yy0bOt5GG7EWSm5I8meRUkl9s9IwbZcCfkY8k+UmSp8Z7MeT/C993khxJ8tr5Xqu05m5W1bq9MfpP3v8A/gC4BHgK2Ltszc3ATxn9Lv+NwC/Xc6bNehu4F38MXD5+f3/nvViy7l8Y/VLALZs99yb+XFwGPAdcOT7+2GbPvYl78bfAt8fvzwBvAJds9uzrsBd/ClwHPHue82vq5npf4XtbhkWr7kVVPV5Vvx4fPsHo9Qxb0ZCfC4CvAz8EXtvI4TbYkL24HXikql4GqKqtuh9D9qKADycJ8CFGwT+3sWOuv6p6jNH3dj5r6uZ6B/98t1y40DVbwYV+n19l9Df4VrTqXiTZAXwJOLSBc22GIT8XVwGXJ/l5kpNJ7tiw6TbWkL24H/g0oxd2PgN8o6re2ZjxLipr6uaQWytMYmq3ZdgCBn+fST7PKPh/sq4TbZ4he/Ed4J6qent0MbdlDdmL7cD1wBeA3wX+LckTVfXieg+3wYbsxReBJ4E/B/4Q+Ock/1pV/73Os11s1tTN9Q6+t2VYNOj7THIN8BCwv6p+tUGzbbQhezELHB3H/grg5iTnqupHGzLhxhn6Z+T1qnoTeDPJY8C1wFYL/pC9uBP4+xo9kT2f5CXgauDfN2bEi8aaurneT+l4W4ZFq+5FkiuBR4CvbMGrt6VW3Yuq2l1Vn6yqTwL/CPzNFow9DPsz8mPgc0m2J/kgo7vVPr/Bc26EIXvxMqN/6ZDk44zuHHl6Q6e8OKypm+t6hV/rd1uG952Be/FN4KPAg+Mr23O1Be8QOHAvWhiyF1X1fJKfAU8D7wAPVdWWu7X4wJ+L+4CHkzzD6GmNe6pqy902OckPgJuAK5IsAN8CPgCTddNbK0hSE77SVpKaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrifwHXe3WluIZOawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_game_scores_breakout_testing)\n",
    "# plt.plot(avg_game_scores_sesh)\n",
    "print(avg_game_scores_spaci_testing)\n",
    "plt.plot([avg_game_scores_sesh.index(i) for i in avg_game_scores_sesh])\n",
    "plt.title('Testing')\n",
    "plt.ylabel('avg scores')\n",
    "plt.xlabel('time')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
