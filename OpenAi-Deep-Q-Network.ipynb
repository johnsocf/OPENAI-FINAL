{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This architectural CNN is from the mind of Deep Mind.\n",
    "def cnn_model():\n",
    "    # This model building approach uses the Keras Functional API.\n",
    "    # The Keras Functional API can be found at: https://keras.io/guides/functional_api/\n",
    "    image_framework = layers.Input(ATARI_IMAGE_SHAPE, name='image-shape-framework')\n",
    "    # Normalize 0-255 image scale to within the constraints of 0 and 1.\n",
    "    # This is done last minute because it is a heavy load to store normalized images, but they\n",
    "    # can more easily be transformed into processing by the model.\n",
    "    normalized_images = layers.Lambda(lambda x: x / 255.0, name='normalize-images')(image_framework)\n",
    "\n",
    "    layer_one_hidden = layers.convolutional.Conv2D(\n",
    "        LAYER_1_SIZE, LAYER_1_FILTER, strides=LAYER_1_STRIDES, activation=ACTIVATION_FUNCTION\n",
    "    )(normalized_images)\n",
    "    layer_two_hidden = layers.convolutional.Conv2D(\n",
    "        LAYER_2_SIZE, LAYER_2_FILTER, strides=LAYER_2_STRIDES, activation=ACTIVATION_FUNCTION\n",
    "    )(layer_one_hidden)\n",
    "    # Flatten before connecting to move to 1D structure.\n",
    "    flat_layer = layers.core.Flatten()(layer_two_hidden)\n",
    "    # Dense layer, fully connected.\n",
    "    fully_connected_layer = layers.Dense(256, activation=ACTIVATION_FUNCTION)(flat_layer)\n",
    "    # Dense layer is fully connected.  This is the layer that is fully connected, mapping to the action probabilities\n",
    "    action_layers = layers.Dense(ACTION_OPTION_COUNT)(fully_connected_layer)\n",
    "    # Action mask encodes.  Format is conforming to the result of action probabilities\n",
    "    action_input = layers.Input((ACTION_OPTION_COUNT,), name='action-mask')\n",
    "    # Multiply layer for each action using the encoded action mask.\n",
    "    # Element-wise multiplication of action mask by action layers (selecting for all actions)\n",
    "    mult_res_layer = layers.Multiply(name='deep-q-cnn')([action_layers, action_input])\n",
    "\n",
    "    model = Model(inputs=[image_framework, action_input], outputs=mult_res_layer)\n",
    "    model.summary()\n",
    "    # RMSProp is commonly used for minibatch learning.\n",
    "    # rho adjusts the influence of the past gradient\n",
    "    optimizer = RMSprop(lr=LEARNING_RATE, rho=0.95, epsilon=REGULATION_SCALE)\n",
    "    model.compile(optimizer, loss=huber_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir():\n",
    "    curr_time = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    record_dir = TRAIN_DIR + \"/openai-atari-{}-logger\".format(curr_time)\n",
    "\n",
    "def init_file_writer_to_local_dir():\n",
    "    return tf.summary.FileWriter(get_log_dir(), tf.get_default_graph())\n",
    "\n",
    "def init_batch_matrix():\n",
    "    return np.zeros((BATCH_SIZE, ATARI_IMAGE_SHAPE[0], ATARI_IMAGE_SHAPE[1], ATARI_IMAGE_SHAPE[2]))\n",
    "\n",
    "def preprocess(observe):\n",
    "    grayscale_imgs = rgb2gray(observe)\n",
    "    shrunk_imgs = resize(grayscale_imgs, ATARI_IMAGE_SHAPE[:2], mode='constant')\n",
    "    processed_observe = np.uint8(shrunk_imgs * 255)\n",
    "    return processed_observe\n",
    "\n",
    "def init_history(observe):\n",
    "    # At start of game, there is no preceding frame.\n",
    "    # So just copy initial states to make a starting state.\n",
    "    state = preprocess(observe)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], ATARI_IMAGE_DIMS_B)\n",
    "    return history\n",
    "\n",
    "def init_test_config(selected_env):\n",
    "    env = gym.make(selected_env)\n",
    "    player_games = 0\n",
    "    episode_number = 0\n",
    "    epsilon = 0.001\n",
    "    global_step = NUM_OBSERVABLE_STEPS + 1\n",
    "    model = load_model(LOGS_FILE_PATH, custom_objects={'huber_loss': huber_loss})\n",
    "    return (env, player_games, episode_number, epsilon, global_step, model)\n",
    "\n",
    "def init_model_clone(model):\n",
    "    # Copy model since actual model weights will get updated later TODO.  when?\n",
    "    # Clone model using keras api function.\n",
    "    model_clone = clone_model(model)\n",
    "    # Clone model weights to new model separately\n",
    "    model_clone.set_weights(model.get_weights())\n",
    "    return model_clone\n",
    "\n",
    "def init_config(selected_env):\n",
    "    env = gym.make(selected_env)\n",
    "    # Deque is imported from collections.  Set to a finite size.  New memory will overwrite old.\n",
    "    memory = deque(maxlen=MAX_MEMORY_SIZE)\n",
    "    # init\n",
    "    epsilon = 1.0\n",
    "    total_steps = 0\n",
    "    # Init at 0.\n",
    "    player_game = 0\n",
    "    return (env, memory, epsilon, total_steps, player_game)\n",
    "\n",
    "def init_metric_config():\n",
    "    # Initialize storage for collecting data on model performance.\n",
    "    total_score = 0\n",
    "    score_interval_count = 0\n",
    "    avg_game_scores = []\n",
    "    max_score = 0\n",
    "    max_scores = []\n",
    "    return (total_score, score_interval_count, avg_game_scores, max_score, max_scores)\n",
    "\n",
    "def init_game_config():\n",
    "    done = False\n",
    "    dead = False\n",
    "    game_step = 0\n",
    "    game_score = 0\n",
    "    game_lives = 5\n",
    "    game_loss = 0.0\n",
    "    return (done, dead, game_step, game_score, game_lives, game_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action is random if it is an observed state or if by chance based on the epsilon threshold, it is.\n",
    "# If action is not random it gets generated from the current model based on history data to this point.\n",
    "# I select the best action from this result.\n",
    "def get_action(history, epsilon, model, is_in_observed_state):\n",
    "    # If is_in_observed_state go with random action, otherwise predict it from the model.\n",
    "    is_below_epsilon_threshole = np.random.rand() <= epsilon\n",
    "    if is_below_epsilon_threshole or is_in_observed_state:\n",
    "        return random.randrange(0, ACTION_OPTION_COUNT)\n",
    "    else:\n",
    "        # get all Q(s, a) possibilities and take largest probability for optimal reward.\n",
    "        q_value = model.predict([history, np.ones(ACTION_OPTION_COUNT).reshape(1, ACTION_OPTION_COUNT)])\n",
    "    # Offset for 0 indexing of the encoding array location of value\n",
    "    return np.argmax(q_value[0])\n",
    "\n",
    "def update_epsilon(total_steps, epsilon):\n",
    "    training = (total_steps > NUM_OBSERVABLE_STEPS)\n",
    "    epsilon_declining = epsilon > EPSILON_MIN\n",
    "    if epsilon_declining and training:\n",
    "        epsilon -= EPSILON_DECAY\n",
    "    return epsilon\n",
    "\n",
    "def find_state_and_history(observed_state, history):\n",
    "    next_state = preprocess(observed_state)\n",
    "    next_state = np.reshape([next_state], ATARI_IMAGE_DIMS_A)\n",
    "    next_history = np.append(next_state, history)\n",
    "    return (next_state, next_history)\n",
    "\n",
    "def find_state_and_history_two(observe, history):\n",
    "    next_state = preprocess(observe)\n",
    "    next_state = np.reshape([next_state], ATARI_IMAGE_DIMS_A)\n",
    "    next_history = np.append(next_state, history[:, :, :, :ACTION_OPTION_COUNT], axis=ACTION_OPTION_COUNT)\n",
    "    return (next_state, next_history)\n",
    "\n",
    "def update_game_lifecycle(game_lives, info):\n",
    "    # Check if the game is over if the agent lost their lives.\n",
    "    game_dead = game_lives > info['ale.lives']\n",
    "    # Update game_lives state to match with what the env. knows.\n",
    "    game_lives = info['ale.lives']\n",
    "    return (game_dead, game_lives)\n",
    "\n",
    "def breakout_from_memory(memory):\n",
    "    training_batch = random.sample(memory, BATCH_SIZE)\n",
    "    history = init_batch_matrix()\n",
    "    next_history = init_batch_matrix()\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    # Memory is stored in: indices 0 = history, 1 = action, 2 = reward, 3 = next_history, 4 = dead\n",
    "    # Index constants:\n",
    "    history_mem_idx = 0\n",
    "    action_mem_idx = 1\n",
    "    reward_mem_idx = 2\n",
    "    next_hist_mem_idx = 3\n",
    "    dead_mem_idx = 4\n",
    "    \n",
    "    \n",
    "     # [:, x] is a np trick for selecting all items for a column at index, 'x'.\n",
    "#     batch = np.array(training_batch)\n",
    "#     action = batch[:, action_mem_idx].tolist()\n",
    "#     reward = batch[:, reward_mem_idx].tolist()\n",
    "#     dead = batch[:, dead_mem_idx].tolist()\n",
    "    \n",
    "    # Reorganize 2D array into category sets by column.\n",
    "    for index, val in enumerate(training_batch):\n",
    "        history[index] = val[history_mem_idx]\n",
    "        next_history[index] = val[next_hist_mem_idx]\n",
    "        action.append(val[action_mem_idx])\n",
    "        reward.append(val[reward_mem_idx])\n",
    "        dead.append(val[dead_mem_idx])\n",
    "        \n",
    "    return (history, next_history, action, reward, dead)\n",
    "\n",
    "def get_one_hot_encoding(targets, nb_classes):\n",
    "#     # clip targets to range within 0 and 2 to make sure they are within action possibilities.\n",
    "#     targets = np.clip(targets, 0, 2)\n",
    "    # array for hot mapping each action\n",
    "    # classes: 3, targets range from 0 to 2.  \n",
    "    # The reshape -1 signifies that data is massaged into a dimension that is unknown.\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "\n",
    "# Huber Loss has a standard formula to maximize loss given complexities in loss variance \n",
    "# depending on the size of an image.\n",
    "# Calculated by using MSE and MAE\n",
    "def huber_loss(a, b):\n",
    "    error = K.abs(a - b)\n",
    "    quadratic_term = K.clip(error, 0.0, 1.0)\n",
    "    linear_term = error - quadratic_term\n",
    "    loss = K.mean(0.5 * K.square(quadratic_term) + linear_term)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Method\n",
    "It is not necessary to run this but the training is so long it is useful to check in on how it is performing.\n",
    "Some iterative logging function should be run in case the model quits while the programmer is sleeping or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_log_stuff(model, total_steps, player_game, score, loss, step, memory, file_writer, selected_env, total_score, score_interval_count, max_score):\n",
    "    # debug.\n",
    "    if player_game % SCREEN_LOG_FREQUENCY == 0:\n",
    "        avg_score = round((total_score/score_interval_count), 2)\n",
    "        print(\"game: {}, total steps: {}, score: {}, avg_score: {}, max_score: {}\".format(player_game, total_steps, score, avg_score, max_score))\n",
    "        \n",
    "    # Record model state iteratively since run time is long.    \n",
    "    if player_game % 1000 == 0 or (player_game + 1) == NUM_GAMES:\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        file_name = \"at-{}-time-{}.h5\".format(now, selected_env)\n",
    "        model_path = os.path.join(TRAIN_DIR, file_name)\n",
    "        model.save(model_path)\n",
    "\n",
    "    # Update File for storing keras stats.\n",
    "    loss_summary = tf.Summary(\n",
    "        value=[tf.Summary.Value(tag=\"loss\", simple_value=loss / float(step))])\n",
    "    file_writer.add_summary(loss_summary, global_step=player_game)\n",
    "\n",
    "    score_summary = tf.Summary(\n",
    "        value=[tf.Summary.Value(tag=\"score\", simple_value=score)])\n",
    "    file_writer.add_summary(score_summary, global_step=player_game)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_future_rewards_from_q(dead, reward, next_q_values):\n",
    "    # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    for i in range(BATCH_SIZE):\n",
    "        if dead[i]:\n",
    "            # If the agent died there is no future reward predicted\n",
    "            q_s_a[i] = reward[i]\n",
    "        else:\n",
    "            # Q(s, a) = r + gamma * max(Q(s', a')).  Bellman's equation for Q value.\n",
    "            # train model for future reward.\n",
    "            q_s_a[i] = reward[i] + GAMMA * np.amax(next_q_values[i])\n",
    "    # Return future rewards for each action for each state in memory.\n",
    "    return q_s_a\n",
    "\n",
    "def train_memory_batch(memory, model, log_dir):\n",
    "    q_s_a = np.zeros((BATCH_SIZE,))\n",
    "    # Reorganize from step to categorical.\n",
    "    history, next_history, action, reward, dead = breakout_from_memory(memory)\n",
    "    # Hot Encode each action as on.\n",
    "    actions_mask = np.ones((BATCH_SIZE, ACTION_OPTION_COUNT))\n",
    "    # Predict for each action with a mask to select for all actions.\n",
    "    next_q_values = model.predict([next_history, actions_mask])\n",
    "    # [q1, q2, q3]\n",
    "     # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    for i in range(BATCH_SIZE):\n",
    "        if dead[i]:\n",
    "            # If the agent died there is no future reward predicted\n",
    "            # TODO: replace with.  q_s_a[i] = reward[i]\n",
    "            q_s_a[i] = -1\n",
    "        else:\n",
    "            # Q(s, a) = r + gamma * max(Q(s', a')).  Bellman's equation for Q value.\n",
    "            # train model for future reward.\n",
    "            q_s_a[i] = reward[i] + GAMMA * np.amax(next_q_values[i])\n",
    "\n",
    "    # Use Bellmans to find future rewards for each memory state, for eac action.\n",
    "    # (indexing stays constant)\n",
    "\n",
    "    # For each action in the batch, encode it using one-hot encoding.\n",
    "    # Resulting array from [0, 1, 2] would be [[1, 0, 0], [0, 1, 0], [0, 0, 1]] for instance.\n",
    "    # print('action', action)\n",
    "    action_one_hot_enc = get_one_hot_encoding(action, ACTION_OPTION_COUNT)\n",
    "    # Multiply the Q(s, a) value into the action matrix using the binary mask.\n",
    "    # The encoded rewards matrix will have the reward for Q(s, a) in place of the 1 in the actions encoding.\n",
    "    rewards_one_hot_enc = action_one_hot_enc * q_s_a[:, None]\n",
    "    \n",
    "    \n",
    "    # Training Data is a tuple of: (history array, actions encoded) by history batch index.\n",
    "    # Label Data is (rewards encoding) by history batch index.\n",
    "    # When predicting, the highest reward will be pulled from the one hot encoded action array.\n",
    "    # TODO: epoch 2\n",
    "    h = model.fit(\n",
    "        [history, action_one_hot_enc], rewards_one_hot_enc, epochs=1,\n",
    "        batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "    return (model, h.history['loss'][0])\n",
    "\n",
    "# This function parents the deep Q Network if the model has enough memory for batch training.\n",
    "def deep_q_iteration_training(memory, total_steps, model_clone, model):\n",
    "    log_dir = None\n",
    "    model, loss = train_memory_batch(memory, model, log_dir)\n",
    "    if total_steps % MODEL_WEIGHTS_REFRESH_THRESOLD == 0:\n",
    "        # Weights on the model clone get piped through so they only get updated as often as \n",
    "        # the treshold dictates the cycle update them.\n",
    "        model_clone.set_weights(model.get_weights())\n",
    "    return (model, model_clone, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mostly this function keeps track of system states, memory, and flags\n",
    "# # It provides the opportunity to create logs for debugging\n",
    "# # Most importantly it takes an action and updates a score.\n",
    "# # It runs training on the model if all observation has been done.  This is Deep Q Learning.\n",
    "def train(selected_env):\n",
    "    # Initialize global states.\n",
    "    # Initialize replay memory D to capacity N.\n",
    "    env, memory, epsilon, total_steps, player_games = init_config(selected_env)\n",
    "    total_score, score_interval_count, avg_game_scores, max_score, max_scores = init_metric_config()\n",
    "    # Initialize storage for collecting data on model performance.\n",
    "    total_score = 0\n",
    "    score_interval_count = 0\n",
    "    avg_game_scores = []\n",
    "    max_score = 0\n",
    "    max_scores = []\n",
    "    # Get a copy of the cnn model with the architecture defined in a separate function.\n",
    "    model = cnn_model()\n",
    "    # Initialize file writer for logging. \n",
    "    \n",
    "    # Initialize file writer.  \n",
    "    # Used for logging and storing the model iteratively to preserve work.\n",
    "    file_writer = init_file_writer_to_local_dir()\n",
    "    \n",
    "    # The main model gets used in the Q learning training, and based on updated weights, \n",
    "    # then also updates the model clone.  \n",
    "    # Targeted Network update.  Initialize action-value function Q with random weights\n",
    "    model_clone = init_model_clone(model)\n",
    "    \n",
    "    # A loop to cover the range of the global number of games played.\n",
    "    # The player games number is kept visible to the program for logging purposes.\n",
    "    while player_games < NUM_GAMES:\n",
    "        # Define global game states.\n",
    "        game_done, player_dead, game_step, game_score, game_lives, game_loss = init_game_config()\n",
    "        # Reset the environment at the beginning of each game.\n",
    "        observe = env.reset()\n",
    "\n",
    "        # Step into the game an arbitrary number of times betw. 1 and a initialization constant.\n",
    "        for _ in range(random.randint(1, INIT_NO_OP_STEPS)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "        \n",
    "        # Prefill the start state with 4 frames.\n",
    "        # Frames are a constructed empty 84 x 84 2D array x 4 frames to create a state.\n",
    "        history = init_history(observe)\n",
    "\n",
    "        # Have the agent play a game given their turn.\n",
    "        while not game_done:\n",
    "            # Set states that define the phases of the observation and training.\n",
    "            is_in_observed_state = (total_steps <= NUM_OBSERVABLE_STEPS)\n",
    "            is_in_training_state = not is_in_observed_state\n",
    "            \n",
    "            # Get an an action for the agent.\n",
    "            action = get_action(history, epsilon, model_clone, is_in_observed_state)\n",
    "            \n",
    "            # Epsilon decays iteratively with the rate of: ((start val - final val)/num games).\n",
    "            epsilon = update_epsilon(total_steps, epsilon)\n",
    "\n",
    "            # Move the agent using the determined action.\n",
    "            # Update it so the game env understands it.\n",
    "            observed_img, reward, game_done, info = env.step(action + 1)\n",
    "            \n",
    "            # Update the score based on the reward from the agent action.\n",
    "            # Move reward to the poles of 1 or -1.\n",
    "            # game_reward = np.clip(reward, -1., 1.)\n",
    "            game_score += reward\n",
    "            \n",
    "            # Preprocess image frame of data returned from the agent's step\n",
    "            # Merge this frame with the three most recent last frames in the history.\n",
    "            # This window of the current frame + three most recent history frames === the current state.\n",
    "            next_state = preprocess(observed_img)\n",
    "            next_state = np.reshape([next_state], ATARI_IMAGE_DIMS_A)\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            \n",
    "            # Get info from env. info on agent status and their current life in the game.\n",
    "            player_dead, game_lives = update_game_lifecycle(game_lives, info)\n",
    "            \n",
    "            # Update the memory with info on the game state. (Store transition)\n",
    "            # The memory is the source from which batch updates draw to make up Q(s, a) for the model.\n",
    "            memory.append((history, action, reward, next_history, player_dead))\n",
    "            \n",
    "            # Deep Q learning begins if the observational state is complete.\n",
    "            # When the model has sufficiently recorded enough memory for training, start batch training.\n",
    "            # Sample random minibatch of transitions from memory for terminal and non-terminal\n",
    "            if is_in_training_state:\n",
    "                model, model_clone, model_loss = deep_q_iteration_training(memory, total_steps, model_clone, model)\n",
    "                game_loss += model_loss\n",
    "                \n",
    "            if not player_dead:\n",
    "                # Update history to include the state if the agent didn't die.\n",
    "                history = next_history\n",
    "            \n",
    "            # Update counts and state flags.\n",
    "            player_dead = False\n",
    "            # These are used more of less for logging and aren't too important to the system.\n",
    "            total_steps += 1\n",
    "            game_step += 1\n",
    "            \n",
    "            if game_done:\n",
    "                # Update the total game score for the metric logs that the function returns.\n",
    "                total_score += game_score\n",
    "                score_interval_count += 1\n",
    "                max_score = max(max_score, game_score)\n",
    "                maybe_log_stuff(model, total_steps, player_games, game_score, game_loss, game_step, memory, file_writer, selected_env, total_score, score_interval_count, max_score)\n",
    "                player_games += 1\n",
    "                # Take a log sample and reset game score counter for the interval.\n",
    "                if player_games % SAMPLE_SCORE_INTERVAL == 0:\n",
    "                    avg_game_score = round((total_score/SAMPLE_SCORE_INTERVAL), 2)\n",
    "                    avg_game_scores.append(avg_game_score)\n",
    "                    max_scores.append(max_score)\n",
    "                    total_score = 0\n",
    "                    score_interval_count = 0\n",
    "    file_writer.close()\n",
    "    return (avg_game_scores, max_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(selected_env, log_file_path):\n",
    "    env = gym.make(selected_env)\n",
    "    player_games = 0\n",
    "    epsilon = 0.001\n",
    "    global_step = NUM_OBSERVABLE_STEPS + 1\n",
    "    model = load_model(log_file_path, custom_objects={'huber_loss': huber_loss})\n",
    "    total_score = 0\n",
    "    avg_game_scores = []\n",
    "\n",
    "    while player_games < NUM_GAMES:\n",
    "        # init variables\n",
    "        done, player_dead, game_step, game_score, game_lives, game_loss = init_game_config()\n",
    "        \n",
    "        observe = env.reset()\n",
    "\n",
    "        # Start with 4 blank initial frames to construct an initial four frame history\n",
    "        observe, _, _, _ = env.step(1)\n",
    "        history = init_history(observe)\n",
    "\n",
    "        while not done:\n",
    "            if RENDER is True:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "\n",
    "            is_in_observed_state = (global_step <= NUM_OBSERVABLE_STEPS)\n",
    "            # get action for the current history and go one step in environment\n",
    "            action = get_action(history, epsilon, model, is_in_observed_state)\n",
    "\n",
    "            observe, reward, done, info = env.step(action)\n",
    "            # pre-process the observation --> history\n",
    "            next_state, next_history = find_state_and_history_two(observe, history)\n",
    "\n",
    "            game_dead, game_lives = update_game_lifecycle(game_lives, info)\n",
    "\n",
    "            # move reward to the poles of 1 or -1 per the deep mind paper's suggestion\n",
    "            game_reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            game_score += game_reward\n",
    "\n",
    "            if not game_dead:\n",
    "                # Update history to include the state if the agent didn't die.\n",
    "                history = next_history\n",
    "\n",
    "            # Update counts and state flags.\n",
    "            dead = False\n",
    "            \n",
    "            if done:\n",
    "                player_games += 1\n",
    "                # update average game score log\n",
    "                total_score += game_score\n",
    "                # take sample\n",
    "                if player_games % SAMPLE_SCORE_INTERVAL == 0:\n",
    "                    avg_game_score = round((total_score/SAMPLE_SCORE_INTERVAL), 2)\n",
    "                    avg_game_scores.append(avg_game_score)\n",
    "                    print('avg: ', avg_game_scores)\n",
    "                    total_score = 0\n",
    "    return avg_game_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from scipy.interpolate import make_interp_spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_meanings(ENV_NAME):\n",
    "  test_env = gym.make(ENV_NAME)\n",
    "  count_meaning = test_env.unwrapped.get_action_meanings()\n",
    "  return count_meaning\n",
    "\n",
    "def get_action_count(ENV_NAME):\n",
    "  return len(get_action_meanings(ENV_NAME)) - 1\n",
    "\n",
    "\n",
    "TRAIN_DIR = 'openai_breakout_training_storage'\n",
    "LOGS_FILE_PATH_BREAKOUT = '/Users/catherinejohnson/projects/CSCI_3202/deepQNetwork/openai_breakout_training_storage/breakout_model_20201128034415.h5'\n",
    "LOGS_FILE_PATH_SPACE_I = '/Users/catherinejohnson/projects/CSCI_3202/deepQNetwork/openai_breakout_training_storage/training_20201129043723.h5'\n",
    "# suggested by Deep Mind Paper\n",
    "ATARI_IMAGE_SHAPE = (84, 84, 4)\n",
    "ATARI_IMAGE_DIMS_A = (1, 84, 84, 1)\n",
    "ATARI_IMAGE_DIMS_B = (1, 84, 84, 4)\n",
    "ATARI_IMAGE_SHAPE = (84, 84, 4)\n",
    "ACTION_OPTION_COUNT = 3\n",
    "RENDER = True\n",
    "# Other Constants\n",
    "MAX_MEMORY_SIZE = 400000\n",
    "ATARI_BREAKOUT_ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "ATARI_SPACE_ENVADERS_ENV_NAME = 'SpaceInvadersDeterministic-v4'\n",
    "ATARI_SKIING_ENV_NAME = 'SkiingDeterministic-v4'\n",
    "ATARI_TENNIS_ENV_NAME = 'TennisDeterministic-v4'\n",
    "QBERT_ENV_NAME = 'QbertDeterministic-v4'\n",
    "PONG_ENV_NAME = 'PongDeterministic-v4'\n",
    "FREEWAY_ENV_NAME = 'FreewayDeterministic-v4'\n",
    "REGULATION_SCALE = 0.01\n",
    "\n",
    "# Constants suggested by Deep Mind Paper\n",
    "LAYER_1_SIZE = 16\n",
    "LAYER_1_FILTER = (8, 8)\n",
    "LAYER_1_STRIDES = (4, 4)\n",
    "LAYER_2_SIZE = 32\n",
    "LAYER_2_FILTER = (4, 4)\n",
    "LAYER_2_STRIDES = (2, 2)\n",
    "ACTIVATION_FUNCTION = 'relu'\n",
    "NUM_GAMES = 100000\n",
    "NUM_OBSERVABLE_STEPS = 50000\n",
    "MODEL_WEIGHTS_REFRESH_THRESOLD = 10000\n",
    "INIT_NO_OP_STEPS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.00025\n",
    "GAMMA = 0.99\n",
    "\n",
    "EPSILON_MIN = 0.1\n",
    "NUM_EPSILON_STEPS = 1000000\n",
    "EPSILON_DECAY = ((1.0 - EPSILON_MIN) / NUM_EPSILON_STEPS)\n",
    "\n",
    "\n",
    "#Overrides to Deep Mind Suggestions\n",
    "NUM_GAMES = 20000\n",
    "# NUM_OBSERVABLE_STEPS = 50000\n",
    "# MODEL_WEIGHTS_REFRESH_THRESOLD = 10000\n",
    "# INIT_NO_OP_STEPS = 10\n",
    "# LEARNING_RATE = 0.1\n",
    "# BATCH_SIZE = 10\n",
    "SAMPLE_SCORE_INTERVAL = 50\n",
    "SCREEN_LOG_FREQUENCY = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'UP', 'DOWN']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_action_meanings(FREEWAY_ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_52\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image-shape-framework (InputLay (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normalize-images (Lambda)       (None, 84, 84, 4)    0           image-shape-framework[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 20, 20, 16)   4112        normalize-images[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 9, 9, 32)     8224        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_52 (Flatten)            (None, 2592)         0           conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 256)          663808      flatten_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_104 (Dense)               (None, 5)            1285        dense_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "action-mask (InputLayer)        (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deep-q-cnn (Multiply)           (None, 5)            0           dense_104[0][0]                  \n",
      "                                                                 action-mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 677,429\n",
      "Trainable params: 677,429\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "game: 0, total steps: 292, score: 125.0, avg_score: 125.0, max_score: 125.0\n",
      "game: 50, total steps: 15538, score: 125.0, avg_score: 125.0, max_score: 575.0\n",
      "game: 100, total steps: 31344, score: 175.0, avg_score: 175.0, max_score: 575.0\n",
      "game: 150, total steps: 47466, score: 200.0, avg_score: 200.0, max_score: 800.0\n",
      "game: 200, total steps: 63168, score: 450.0, avg_score: 450.0, max_score: 800.0\n",
      "game: 250, total steps: 79098, score: 75.0, avg_score: 75.0, max_score: 800.0\n"
     ]
    }
   ],
   "source": [
    "# NUM_OBSERVABLE_STEPS = 500\n",
    "# SAMPLE_SCORE_INTERVAL = 10\n",
    "# SCREEN_LOG_FREQUENCY = 10\n",
    "\n",
    "ENV_NAME = QBERT_ENV_NAME\n",
    "ACTION_OPTION_COUNT = get_action_count(ENV_NAME)\n",
    "avg_game_scores_qbert_training, max_qbert_score_trend = train(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image-shape-framework (InputLay (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normalize-images (Lambda)       (None, 84, 84, 4)    0           image-shape-framework[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 20, 20, 16)   4112        normalize-images[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 9, 9, 32)     8224        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 2592)         0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 256)          663808      flatten_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 3)            771         dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "action-mask (InputLayer)        (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deep-q-cnn (Multiply)           (None, 3)            0           dense_60[0][0]                   \n",
      "                                                                 action-mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 676,915\n",
      "Trainable params: 676,915\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "game: 0, total steps: 102, score: 0.0, avg_score: 0.0, max_score: 0\n",
      "game: 50, total steps: 8009, score: 3.0, avg_score: 3.0, max_score: 6.0\n",
      "game: 100, total steps: 16121, score: 1.0, avg_score: 1.0, max_score: 6.0\n",
      "game: 150, total steps: 24169, score: 1.0, avg_score: 1.0, max_score: 6.0\n",
      "game: 200, total steps: 32421, score: 1.0, avg_score: 1.0, max_score: 6.0\n",
      "game: 250, total steps: 40271, score: 0.0, avg_score: 0.0, max_score: 6.0\n",
      "game: 300, total steps: 48506, score: 0.0, avg_score: 0.0, max_score: 6.0\n",
      "game: 350, total steps: 56575, score: 0.0, avg_score: 0.0, max_score: 6.0\n",
      "game: 400, total steps: 64623, score: 0.0, avg_score: 0.0, max_score: 6.0\n",
      "game: 450, total steps: 74899, score: 8.0, avg_score: 8.0, max_score: 8.0\n",
      "game: 500, total steps: 84275, score: 0.0, avg_score: 0.0, max_score: 8.0\n",
      "game: 550, total steps: 92118, score: 0.0, avg_score: 0.0, max_score: 9.0\n",
      "game: 600, total steps: 99716, score: 0.0, avg_score: 0.0, max_score: 9.0\n",
      "game: 650, total steps: 113205, score: 0.0, avg_score: 0.0, max_score: 9.0\n",
      "game: 700, total steps: 122919, score: 4.0, avg_score: 4.0, max_score: 9.0\n",
      "game: 750, total steps: 139729, score: 4.0, avg_score: 4.0, max_score: 9.0\n",
      "game: 800, total steps: 157175, score: 4.0, avg_score: 4.0, max_score: 9.0\n",
      "game: 850, total steps: 175034, score: 4.0, avg_score: 4.0, max_score: 9.0\n",
      "game: 900, total steps: 191015, score: 0.0, avg_score: 0.0, max_score: 9.0\n",
      "game: 950, total steps: 202712, score: 0.0, avg_score: 0.0, max_score: 9.0\n",
      "game: 1000, total steps: 215229, score: 0.0, avg_score: 0.0, max_score: 9.0\n",
      "game: 1050, total steps: 230150, score: 5.0, avg_score: 5.0, max_score: 9.0\n",
      "game: 1100, total steps: 247468, score: 3.0, avg_score: 3.0, max_score: 9.0\n",
      "game: 1150, total steps: 264129, score: 3.0, avg_score: 3.0, max_score: 9.0\n",
      "game: 1200, total steps: 281943, score: 4.0, avg_score: 4.0, max_score: 9.0\n",
      "game: 1250, total steps: 299704, score: 3.0, avg_score: 3.0, max_score: 9.0\n",
      "game: 1300, total steps: 314596, score: 4.0, avg_score: 4.0, max_score: 9.0\n",
      "game: 1350, total steps: 324729, score: 0.0, avg_score: 0.0, max_score: 9.0\n",
      "game: 1400, total steps: 333307, score: 5.0, avg_score: 5.0, max_score: 9.0\n",
      "game: 1450, total steps: 351085, score: 5.0, avg_score: 5.0, max_score: 9.0\n",
      "game: 1500, total steps: 367981, score: 5.0, avg_score: 5.0, max_score: 9.0\n",
      "game: 1550, total steps: 380492, score: 0.0, avg_score: 0.0, max_score: 9.0\n",
      "game: 1600, total steps: 388894, score: 3.0, avg_score: 3.0, max_score: 9.0\n",
      "game: 1650, total steps: 405003, score: 3.0, avg_score: 3.0, max_score: 9.0\n",
      "game: 1700, total steps: 418494, score: 0.0, avg_score: 0.0, max_score: 9.0\n",
      "game: 1750, total steps: 433785, score: 0.0, avg_score: 0.0, max_score: 10.0\n",
      "game: 1800, total steps: 444152, score: 2.0, avg_score: 2.0, max_score: 10.0\n",
      "game: 1850, total steps: 456584, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 1900, total steps: 474685, score: 5.0, avg_score: 5.0, max_score: 10.0\n",
      "game: 1950, total steps: 491785, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 2000, total steps: 506789, score: 5.0, avg_score: 5.0, max_score: 10.0\n",
      "game: 2050, total steps: 522608, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 2100, total steps: 538828, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 2150, total steps: 557137, score: 6.0, avg_score: 6.0, max_score: 10.0\n",
      "game: 2200, total steps: 573708, score: 8.0, avg_score: 8.0, max_score: 10.0\n",
      "game: 2250, total steps: 590954, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 2300, total steps: 609428, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 2350, total steps: 622547, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 2400, total steps: 640266, score: 8.0, avg_score: 8.0, max_score: 10.0\n",
      "game: 2450, total steps: 658766, score: 2.0, avg_score: 2.0, max_score: 10.0\n",
      "game: 2500, total steps: 676011, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 2550, total steps: 693413, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 2600, total steps: 711278, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 2650, total steps: 727653, score: 5.0, avg_score: 5.0, max_score: 10.0\n",
      "game: 2700, total steps: 740987, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 2750, total steps: 757127, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 2800, total steps: 773482, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 2850, total steps: 784700, score: 0.0, avg_score: 0.0, max_score: 10.0\n",
      "game: 2900, total steps: 794759, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 2950, total steps: 810585, score: 0.0, avg_score: 0.0, max_score: 10.0\n",
      "game: 3000, total steps: 817042, score: 2.0, avg_score: 2.0, max_score: 10.0\n",
      "game: 3050, total steps: 830320, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 3100, total steps: 845144, score: 0.0, avg_score: 0.0, max_score: 10.0\n",
      "game: 3150, total steps: 860027, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 3200, total steps: 876319, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 3250, total steps: 889164, score: 0.0, avg_score: 0.0, max_score: 10.0\n",
      "game: 3300, total steps: 905498, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 3350, total steps: 923434, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 3400, total steps: 940384, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 3450, total steps: 955746, score: 1.0, avg_score: 1.0, max_score: 10.0\n",
      "game: 3500, total steps: 969301, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 3550, total steps: 985161, score: 0.0, avg_score: 0.0, max_score: 10.0\n",
      "game: 3600, total steps: 994945, score: 0.0, avg_score: 0.0, max_score: 10.0\n",
      "game: 3650, total steps: 1006874, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 3700, total steps: 1024974, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 3750, total steps: 1042680, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 3800, total steps: 1056813, score: 0.0, avg_score: 0.0, max_score: 10.0\n",
      "game: 3850, total steps: 1071762, score: 5.0, avg_score: 5.0, max_score: 10.0\n",
      "game: 3900, total steps: 1089868, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 3950, total steps: 1106517, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 4000, total steps: 1124562, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 4050, total steps: 1141499, score: 5.0, avg_score: 5.0, max_score: 10.0\n",
      "game: 4100, total steps: 1158355, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 4150, total steps: 1170395, score: 4.0, avg_score: 4.0, max_score: 10.0\n",
      "game: 4200, total steps: 1188333, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 4250, total steps: 1204833, score: 5.0, avg_score: 5.0, max_score: 10.0\n",
      "game: 4300, total steps: 1222380, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 4350, total steps: 1239001, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 4400, total steps: 1256648, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 4450, total steps: 1274502, score: 5.0, avg_score: 5.0, max_score: 10.0\n",
      "game: 4500, total steps: 1291584, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 4550, total steps: 1308360, score: 5.0, avg_score: 5.0, max_score: 10.0\n",
      "game: 4600, total steps: 1324414, score: 0.0, avg_score: 0.0, max_score: 10.0\n",
      "game: 4650, total steps: 1340029, score: 3.0, avg_score: 3.0, max_score: 10.0\n",
      "game: 4700, total steps: 1352175, score: 4.0, avg_score: 4.0, max_score: 10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-f4b2ce5fc483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavg_game_scores_breakout_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_score_trend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATARI_BREAKOUT_ENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-190-a796f330e201>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(selected_env)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# When the model has sufficiently recorded enough memory for training, start batch training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_in_training_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_clone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_q_iteration_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_clone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mgame_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-cb3ba1d05590>\u001b[0m in \u001b[0;36mdeep_q_iteration_training\u001b[0;34m(memory, total_steps, model_clone, model)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeep_q_iteration_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_clone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mMODEL_WEIGHTS_REFRESH_THRESOLD\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Weights on the model clone get piped through so they only get updated as often as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-cb3ba1d05590>\u001b[0m in \u001b[0;36mtrain_memory_batch\u001b[0;34m(memory, model, log_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mq_s_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Reorganize from step to categorical.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbreakout_from_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Hot Encode each action as on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mactions_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION_OPTION_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-187-a9097c87dc7f>\u001b[0m in \u001b[0;36mbreakout_from_memory\u001b[0;34m(memory)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Reorganize 2D array into category sets by column.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory_mem_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mnext_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_hist_mem_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_mem_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ENV_NAME = ATARI_BREAKOUT_ENV_NAME\n",
    "ACTION_OPTION_COUNT = get_action_count(ENV_NAME)\n",
    "avg_game_scores_breakout_training, max_score_trend = train(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improvement isn't expected to be linearly continuous because it is still exploring at times.\n",
    "# 1. divide data into groups of 10\n",
    "# 2. find avg.  use this.\n",
    "# 3.  curr 4750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_game_scores_spaci_training, max_score_trend = train(ATARI_SPACE_ENVADERS_ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Step: Manually update file in RESTORE_FILE_PATH for testing.\n",
    "avg_game_scores_breakout_testing = test(ATARI_BREAKOUT_ENV_NAME, LOGS_FILE_PATH_BREAKOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_game_scores_spaci_testing = test(ATARI_SPACE_ENVADERS_ENV_NAME, LOGS_FILE_PATH_SPACE_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.84, 1.85, 1.68, 1.67, 1.66, 1.85, 1.87, 1.9, 1.68, 1.68]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_game_scores_breakout_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "x_len = len(avg_game_scores_breakout_training)\n",
    "time_series = [i for i in range(0, x_len)]\n",
    "x_len_minus = x_len-1\n",
    "\n",
    "x = np.array(time_series[:x_len_minus])\n",
    "y_b = np.array(avg_game_scores_breakout_training[:x_len_minus])\n",
    "#y_s = np.array(avg_game_scores_spaci_training[:x_len_minus])\n",
    "x_new = np.linspace(0, len(time_series), 10000)\n",
    "\n",
    "a_BSpline_b = make_interp_spline(x, y_b)\n",
    "#a_BSpline_s = make_interp_spline(x, y_s)\n",
    "\n",
    "y_new_b = a_BSpline_b(x_new)\n",
    "#y_new_s = a_BSpline_s(x_new)\n",
    "\n",
    "plt.plot(x_new, y_new_b)\n",
    "#plt.plot(x_new, y_new_s)\n",
    "\n",
    "print(avg_game_scores_breakout_training)\n",
    "#plt.plot(time_series)\n",
    "plt.title('Training')\n",
    "plt.ylabel('avg scores')\n",
    "plt.xlabel('time')\n",
    "plt.legend(['space invaders', 'breakout'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fist 4700 games of breakout recorded, but thread never finished before interruption so copied them over.\n",
    "x = [0, 3.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 4.0, 4.0, 4.0, 4.0, 0.0, 0.0]\n",
    "y = [0.0, 5.0, 3.0, 3.0, 4.0, 3.0, 4.0, 0.0, 5.0, 5.0, 5.0, 0.0, 3.0, 3.0, 0.0, 0.0, 2.0, 4.0, 5.0]\n",
    "z = [10.0, 3.0, 5.0, 4.0, 3.0, 6.0, 8.0, 3.0, 4.0, 4.0, 8.0, 2.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0]\n",
    "a = [0.0, 3.0, 0.0, 2.0, 3.0, 0.0, 4.0, 4.0, 0.0, 4.0, 4.0, 4.0, 1.0, 3.0, 0.0, 0.0, 3.0, 3.0, 3.0]\n",
    "b = [0.0, 5.0, 4.0, 3.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 3.0, 3.0, 5.0, 3.0, 5.0, 0.0, 3.0, 4.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET_1 = x + y + z + a + b\n",
    "TEST_SET_2 = \n",
    "len_1 = [i for i, v in enumerate(TEST_SET_1)]\n",
    "len_2 = [i for i, v in enumerate(TEST_SET_1)]\n",
    "x = np.array(len_1)\n",
    "y = np.array(TEST_SET_1)\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "\n",
    "x_b = np.array(len_2)\n",
    "y_b = np.array(TEST_SET_1)\n",
    "m_b, b_b = np.polyfit(x_b, y_b, 1)\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "\n",
    "plt.plot(x_b, y_b, 'r')\n",
    "\n",
    "plt.plot(x, m*x + b)\n",
    "plt.plot(x_b, m_b*x_b + b_b)\n",
    "plt.title('Training')\n",
    "plt.ylabel('avg scores')\n",
    "plt.xlabel('time')\n",
    "plt.legend(['breakout', 'qbert'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_game_scores_breakout_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_game_scores_breakout_testing)\n",
    "plt.plot(avg_game_scores_spaci_testing)\n",
    "print(avg_game_scores_breakout_testing)\n",
    "plt.title('Testing')\n",
    "plt.ylabel('avg scores')\n",
    "plt.xlabel('time')\n",
    "plt.legend(['space invaders', 'breakout'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training three commits back."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
